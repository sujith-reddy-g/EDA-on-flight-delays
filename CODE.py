# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1solQncEX9ghVl5CQv4BXhMg3dgV_nK8o

////////////////////////////////////////////////////////////////////////////////This is BIG DATA ANALYTICS project ////////////////////////////////////////////////////////////////////////

Sujith Reddy Ganta

First we have downloaded datasets to our local system then uploaded to the datasets to our google drive. To use the datasets we have mounted to the google colab for performing EDA. thus is the location '/content/drive/MyDrive/itc686_project/' if you are running the code would request to place the files in the same loaction.<br>

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
"""

from google.colab import drive
drive.mount('/content/drive')

!ls drive/MyDrive/itc686_project/Flights_202*

!ls

!pip install pyspark

import pyspark
print(pyspark.__version__)

from pyspark.sql import SparkSession
from pyspark.sql.types import *


spark = SparkSession.builder \
    .appName("project") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.memory.fraction", "0.6") \
    .config("spark.memory.storageFraction", "0.5") \
    .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC") \
    .config("spark.driver.extraJavaOptions", "-XX:+UseG1GC") \
    .master("local[4]") \
    .getOrCreate()

# spark.stop()   #for stoping the sql session

from pyspark.sql.functions import col, when, count, mean, sum , isnan, lit,floor,round
import matplotlib.pyplot as plt
from pyspark.sql.functions import corr,avg,mean,stddev,rank,max,array, sort_array
from pyspark.sql.types import *
from pyspark.sql import Row
import pandas as pd
from scipy.stats import mannwhitneyu, wilcoxon
from scipy import stats
from pyspark.sql.window import Window
import seaborn as sns
import matplotlib.pyplot as plt

df_all = spark.read.csv('/content/drive/MyDrive/itc686_project/Flights_202*', header=True)

df_all.count()       # initial number of records

columns = df_all.columns
len(columns)      # number of columns

df_selective = df_all['Year', 'Month', 'DayofMonth', 'DayOfWeek',
            'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime',
            'Marketing_Airline_Network', 'Flight_Number_Marketing_Airline',
            'Tail_Number', 'ActualElapsedTime', 'CRSElapsedTime',
            'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest',
            'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled',
            'CancellationCode','CarrierDelay',
            'WeatherDelay', 'NASDelay', 'SecurityDelay',
            'LateAircraftDelay','Operating_Airline ','DepDelayMinutes']
selected = df_selective.columns
len(selected)      # selected number of columns

df_selective.printSchema()    # initila schema

# changiing schema accordingly
schema_types = {
    "Year": IntegerType(),
    "Month": IntegerType(),
    "DayofMonth": IntegerType(),
    "DayOfWeek": IntegerType(),
    "DepTime": IntegerType(),
    "CRSDepTime": IntegerType(),
    "ArrTime": IntegerType(),
    "CRSArrTime": IntegerType(),
    "Marketing_Airline_Network": StringType(),
    "Flight_Number_Marketing_Airline": IntegerType(),
    "Tail_Number": StringType(),
    "ActualElapsedTime": IntegerType(),
    "CRSElapsedTime": IntegerType(),
    "AirTime": IntegerType(),
    "ArrDelay": IntegerType(),
    "DepDelay": IntegerType(),
    "Origin": StringType(),
    "Dest": StringType(),
    "Distance": IntegerType(),
    "TaxiIn": IntegerType(),
    "TaxiOut": IntegerType(),
    "Cancelled": IntegerType(),
    "CancellationCode": StringType(),
    "CarrierDelay": IntegerType(),
    "WeatherDelay": IntegerType(),
    "NASDelay": IntegerType(),
    "SecurityDelay": IntegerType(),
    "LateAircraftDelay": IntegerType(),
    'Operating_Airline ': StringType(),
    'DepDelayMinutes':IntegerType()
}

for column, dtype in schema_types.items():
    if isinstance(dtype, IntegerType):
        df_selective = df_selective.withColumn(column, col(column).cast("int"))

df_selective.printSchema()   # changed the inital schma

df_selective=df_selective.dropna(how='any',subset=['Tail_Number','Origin','Dest',"DepDelay", "ArrDelay",'AirTime']) # cleaning the data set by removing the records with useful columns null
df_delay_pov = df_selective.filter((col('ArrDelay')>=0) & (col('DepDelay')>=0) )
df_cancelled=df_all.filter((col('Cancelled') == 1) & ~col('CancellationCode').isNull()) # seggrigating the data set which are cancelled and have cancellation code

df_cancelled.count()

df_selective.count() # count after cleaning

"""### **Hypothesis 1**"""

data = df_delay_pov.select("DepDelay", "ArrDelay").dropna().collect()

dep_delays = [row['DepDelay'] for row in data]
arr_delays = [row['ArrDelay'] for row in data]

delay_threshold = 120  # setting threshold to 120 for better visuallization of the graph
filtered_data = [(dep, arr) for dep, arr in data if dep < delay_threshold and arr < delay_threshold]

dep_delays_filtered, arr_delays_filtered = zip(*filtered_data)

# Plot
plt.scatter(dep_delays_filtered, arr_delays_filtered, alpha=0.5)
plt.scatter(dep_delays, arr_delays, alpha=0.5)
plt.title('Departure Delay vs Arrival Delay')
plt.xlabel('Departure Delay (minutes)')
plt.ylabel('Arrival Delay (minutes)')
plt.grid(True)
plt.show()

"""### **Hypothesis 2**"""

# Summary of delays and Ait Time
df_delay_pov.select("DepDelay", "ArrDelay", "AirTime").summary("count", "mean", "stddev", "min", "max").show()

arr_delay_data = df_delay_pov.select("ArrDelay").rdd.flatMap(lambda x: x).collect()
dep_delay_data = df_delay_pov.select("DepDelay").rdd.flatMap(lambda x: x).collect()
arr_data = df_delay_pov.select("AirTime").rdd.flatMap(lambda x: x).collect()

sns.set(style="whitegrid")

# Plot
plt.figure(figsize=(12, 6))
sns.histplot(arr_delay_data, bins=500, kde=True, color='blue')
plt.title('Histogram and Distribution Plot of Arrival Delays')
plt.xlabel('Arrival Delay (minutes)')
plt.ylabel('Frequency')
plt.xlim(-50, 500)  # Set limits for x-axis
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(dep_delay_data, bins=500, kde=True, color='blue')
plt.title('Histogram and Distribution Plot of depart Delay')
plt.xlabel('depart Delay (minutes)')
plt.ylabel('Frequency')
plt.xlim(-50, 500)  # Set limits for x-axis
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(arr_data, bins=30, kde=True, color='blue')
plt.title('Histogram and Distribution Plot of Air Time')
plt.xlabel('Air Time (minutes)')
plt.ylabel('Frequency')
plt.xlim(-50, 500)  # Set limits for x-axis
plt.show()

"""### **Hypothesis 3**"""

cancellation_counts_df = df_cancelled.groupBy("CancellationCode").count().dropna().collect()
print(cancellation_counts_df)

code = [row['CancellationCode'] for row in cancellation_counts_df]
count = [row['count'] for row in cancellation_counts_df]

# A: Carrier-caused cancellation
# B: Weather-related cancellation
# C: National Aviation System (NAS)-related cancellation (e.g., non-weather-related airport issues, air traffic control issues, heavy airport traffic, etc.)
# D: Security-related cancellation


plt.bar(code, count)
plt.title('Distribution of Flight Cancellation Reasons')
plt.xlabel('Cancellation Code')
plt.ylabel('Number of Flights')
plt.show()

"""### **Hypothesis 4**"""

airline_network_counts = df_selective.groupBy("Marketing_Airline_Network").count()

total_flights = airline_network_counts.groupBy().sum("count").collect()[0][0]
print(total_flights)
threshold = 0.05 * total_flights

airline_network_counts_filtered = airline_network_counts.filter(col("count") >= threshold)

other_flights_count = airline_network_counts.filter(col("count") < threshold).groupBy().sum("count").collect()[0][0]

other_row = Row("Marketing_Airline_Network", "count")("Other", other_flights_count)
other_df = spark.createDataFrame([other_row])

final_airline_network_counts = airline_network_counts_filtered.union(other_df)

airline_network_counts_pandas = final_airline_network_counts.toPandas().set_index("Marketing_Airline_Network")
full_forms = {
    'UA': 'United Airlines',
    'WN': 'Southwest Airlines',
    'AS': 'Alaska Airlines',
    'DL': 'Delta Air Lines',
    'AA': 'American Airlines',
    'Other': 'Other Airlines'
}

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 8))
plt.pie(airline_network_counts_pandas['count'], labels=airline_network_counts_pandas.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Flights Among Airline Networks')
plt.legend([full_forms[label] for label in airline_network_counts_pandas.index], loc="lower right", title="Airlines")

plt.show()

"""### **Hypothesis 5**"""

day_of_week_counts = df_selective.groupBy("DayOfWeek").count().orderBy("DayOfWeek").collect()

# Prepare data for plotting
days = [row['DayOfWeek'] for row in day_of_week_counts]
counts = [row['count'] for row in day_of_week_counts]
# Plotting a bar chart to visualize the distribution of flights across days of the week
plt.bar(days, counts)
plt.title('Distribution of Flights Across Days of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Number of Flights')
plt.xticks(range(1, 8), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], rotation=90)
plt.show()

"""### **Hypothesis 6**"""

y_cancelled = df_cancelled.filter((col('CancellationCode') == 'A')).groupBy("Marketing_Airline_Network", "Operating_Airline ").count()

diff= y_cancelled.filter((col('Marketing_Airline_Network')!=col('Operating_Airline '))).groupBy(col('Marketing_Airline_Network')).agg({'count': 'sum'}).withColumnRenamed("sum(count)", "diff")
same= y_cancelled.filter(col('Marketing_Airline_Network')==col('Operating_Airline ')).groupBy(col('Marketing_Airline_Network')).agg({'count': 'sum'}).withColumnRenamed("sum(count)", "same")

same_diff= same.join(diff,on='Marketing_Airline_Network',how="inner").collect()

print(same_diff)

import numpy as np
airlines = [row['Marketing_Airline_Network'] for row in same_diff]
same = [row['same'] for row in same_diff]
diff = [row['diff'] for row in same_diff]

# Data for stacked area chart
y = np.vstack([same, diff])

fig, ax = plt.subplots()
ax.stackplot(airlines, y, labels=['Same', 'Different'], colors=['b', 'r'])
ax.legend(loc='upper right')
ax.set_title('Stacked Area Chart of Airline Operations')
ax.set_xlabel('Marketing Airline Network')
ax.set_ylabel('Counts')

plt.show()

t_statistic, p_value = stats.ttest_rel(same, diff)

print("T-statistic:", t_statistic)
print("P-value:", p_value)

columns = ["Marketing_Airline_Network", "same", "diff"]
df = spark.createDataFrame(same_diff)

df = df.toPandas()

# Wilcoxon Signed-Rank Test for paired samples
# Note: Only use this test if the samples are indeed paired.
stat, p_value = wilcoxon(df['same'], df['diff'])
print(f"Wilcoxon Signed-Rank Test: Stat={stat}, P-value={p_value}")

"""### **query 1**"""

freq_dest=df_selective.groupBy("Dest").count().orderBy(col('count').desc())

freq_dest.show(5)

"""### **query 2**"""

df_sorted = df_selective.withColumn("SortedPair", sort_array(array("Origin", "Dest")))
distinct_count = df_sorted.select("SortedPair").distinct()
print("Number of distinct routes:", distinct_count.count())

distinct_count.show(5)

# Compute Pearson correlation coefficient for statistical evidance to support the relation between the "DepDelay" and "ArrDelay"
correlation_df = df_delay_pov.select(corr("DepDelay", "ArrDelay").alias("correlation"))
correlation_df.show()

"""### **query 3**"""

from pyspark.sql.functions import array, col, lit, sort_array

df_sorted = df_cancelled.withColumn("SortedPair_c", sort_array(array("Origin", "Dest")))    # identyfying
distinct_cancel_count = df_sorted.select("SortedPair_c").distinct()
print("Number of distinct routes:", distinct_cancel_count.count())

distinct_cancel_count.show(5)

unique_df_cancelled = distinct_count.join(
    distinct_cancel_count,
    on=distinct_count["SortedPair"] == distinct_cancel_count["SortedPair_c"],
    how="left_anti"
)
result_df = unique_df_cancelled.select(distinct_count["SortedPair"])
result_df.show()

result_df.count()

"""### **query 4**"""

df_selective.filter(
    (col("ActualElapsedTime").isNotNull()) &
    (col("CRSElapsedTime").isNotNull())).count()

result_df = df_selective.groupBy("Origin", "Dest")\
.agg(round(avg(col("CRSElapsedTime")/col("ActualElapsedTime")), 1).alias("EfficiencyRatio"))\
.filter(col("EfficiencyRatio") > 0)

result_df.orderBy(col("EfficiencyRatio").desc()).show(5)

result_df.orderBy(col("EfficiencyRatio").asc()).show(5)

"""### **query 5**"""

from pyspark.sql import functions as F

result_df = df_selective.groupBy("Origin").agg(
    F.count("*").alias("TotalDepartures"),
    F.round(F.avg("TaxiOut"), 1).alias("AvgTaxiOutTime")
)

windowSpec = Window.orderBy(col("AvgTaxiOutTime").asc())

result_df = result_df.withColumn("TaxiOutEfficiencyRank", rank().over(windowSpec))\
            .orderBy(col("TaxiOutEfficiencyRank").asc()).show()

"""### **query 6**"""

origin_delays = df_delay_pov.select(
    col("Origin").alias("Airport"),
    "CarrierDelay","WeatherDelay",
    "NASDelay","SecurityDelay",
    "LateAircraftDelay")

dest_delays = df_delay_pov.select(
    col("Dest").alias("Airport"),
    "CarrierDelay","WeatherDelay",
    "NASDelay","SecurityDelay",
    "LateAircraftDelay")

airport_delays = origin_delays.unionAll(dest_delays)

avg_delays = airport_delays.groupBy("Airport").agg(
    round(avg(when(col("CarrierDelay") > 0, col("CarrierDelay")))).alias("AvgCarrierDelay"),
    round(avg(when(col("WeatherDelay") > 0, col("WeatherDelay")))).alias("AvgWeatherDelay"),
    round(avg(when(col("NASDelay") > 0, col("NASDelay")))).alias("AvgNASDelay"),
    round(avg(when(col("SecurityDelay") > 0, col("SecurityDelay")))).alias("AvgSecurityDelay"),
    round(avg(when(col("LateAircraftDelay") > 0, col("LateAircraftDelay")))).alias("AvgLateAircraftDelay"))

avg_delays.show()

"""### **query 7**"""

from pyspark.sql.functions import col, max ,min



result_df = df_selective.groupBy("Origin", "Dest").agg(
    min(col("AirTime")).alias("MaxAirTime"))

df_selective = df_selective.withColumnRenamed("Origin", "Origin_selective")
df_selective = df_selective.withColumnRenamed("Dest", "Dest_selective")
df_selective = df_selective.withColumnRenamed("Operating_Airline ", "Operating_Airline")

joined_df = result_df.join(
    df_selective,
    (result_df.Origin == df_selective.Origin_selective) &
    (result_df.Dest == df_selective.Dest_selective) &
    (result_df.MaxAirTime == df_selective.AirTime),
    how="inner")

joined_df.select(
    df_selective.Origin_selective, df_selective.Dest_selective,
    df_selective.Operating_Airline).show()

df_selective = df_selective.withColumnRenamed("Origin_selective","Origin")
df_selective = df_selective.withColumnRenamed("Dest_selective","Dest")
df_selective = df_selective.withColumnRenamed("Operating_Airline","Operating_Airline ")

df_selective.select('Origin','Dest','Operating_Airline ','AirTime').groupBy("Origin", "Dest")\
.agg(min(col("AirTime")).alias("Min_AirTime")).show()

"""### **query 8**"""

airline_performance = df_selective.groupBy("Marketing_Airline_Network", "Origin").agg(
    F.count("*").alias("TotalFlights"),
    (F.sum(F.when(F.col("ArrDelay") <= 15, 1).otherwise(0)) / F.count("*")).alias("OnTimeRate")
)

# Window for market share
windowSpecMarketShare = Window.partitionBy("Origin")
market_share = df_selective.groupBy("Marketing_Airline_Network", "Origin").agg(
    (F.count("*") * 100.0 / F.sum(F.count("*")).over(windowSpecMarketShare)).alias("MarketSharePercentage")
)

# Joining DataFrames
result = airline_performance.alias("a").join(
    market_share.alias("m"),
    (col("a.Marketing_Airline_Network") == col("m.Marketing_Airline_Network")) &
    (col("a.Origin") == col("m.Origin"))
)

# Window for ranking by OnTimeRate
windowSpecRank = Window.partitionBy("a.Origin").orderBy(col("a.OnTimeRate").desc())

# Rank airlines by OnTimeRate within each airport
result_with_rank = result.withColumn("rank", rank().over(windowSpecRank))

# Filter for top rank
result_top = result_with_rank.filter(col("rank") == 1)

# Select necessary columns
final_result = result_top.select(
    "a.Marketing_Airline_Network",
    "a.Origin",
    "a.OnTimeRate",
    # "m.MarketSharePercentage"
).orderBy("a.Origin", col("a.OnTimeRate").desc())

# Show the results
final_result.show()

"""### **query 9**"""

df_all_delay = df_selective.filter(col('Cancelled')==0)

from pyspark.sql import functions as F

df_all_delay = df_all_delay.withColumn(
    'Status',
    when(col('DepDelayMinutes') == 0, 'OnTime')
    .when((col('DepDelayMinutes') > 0) & (col('DepDelayMinutes') <= 15), 'SlightDelay')
    .when((col('DepDelayMinutes') > 15) & (col('DepDelayMinutes') <= 45), 'MediumDelay')
    .when(col('DepDelayMinutes') > 45, 'LargeDelay'))
# Showing the DataFrame with the new 'Status' column
df_all_delay.show()

from pyspark.sql import functions as F

df_all_delay = df_all_delay.withColumn(
    'Status',
    when(col('DepDelayMinutes') == 0, 'OnTime')
    .when((col('DepDelayMinutes') > 0) & (col('DepDelayMinutes') <= 15), 'SlightDelay')
    .when((col('DepDelayMinutes') > 15) & (col('DepDelayMinutes') <= 45), 'MediumDelay')
    .when(col('DepDelayMinutes') > 45, 'LargeDelay'))
dfairline = df_all_delay.groupBy('Operating_Airline ').pivot('Status').count()
dfairline = dfairline.withColumn("TotalFlights", col("LargeDelay") + col("MediumDelay") + col("OnTime") + col("SlightDelay"))
airline_delay = dfairline.withColumn("LargeDelay", col("LargeDelay") / col("TotalFlights")) \
                     .withColumn("MediumDelay", col("MediumDelay") / col("TotalFlights")) \
                     .withColumn("OnTime", col("OnTime") / col("TotalFlights")) \
                     .withColumn("SlightDelay", col("SlightDelay") / col("TotalFlights"))
airline_delay=airline_delay.select('Operating_Airline ','OnTime','SlightDelay','MediumDelay','LargeDelay').orderBy(col('OnTime').desc())


total_flights = dfairline.select([F.sum(c).alias(c) for c in dfairline.columns if c != 'Operating_Airline '])

total_flights.show()

airline_delay=airline_delay.select('Operating_Airline ','OnTime','SlightDelay','MediumDelay','LargeDelay').orderBy(col('OnTime').desc())

pandas_df = airline_delay.toPandas()

# Using styling in pandas to add a gradient
styled_df = pandas_df.style.background_gradient(subset=['OnTime', 'SlightDelay', 'MediumDelay', 'LargeDelay'], cmap='YlOrRd')
styled_df